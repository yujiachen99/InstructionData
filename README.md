## What makes good instruction data for  LLM Fine-tuning on code intelligence tasksï¼Ÿ

This repository contains code and resources for the empirical study on how various instruction data features affect the performance of instruction fine-tuning (IFT) for code large language models (LLMs). The study explores four key research questions (RQs) regarding domain composition, semantic diversity, problem complexity, and task-type coverage.

### ğŸ“ Project Structure

```bash

â”œâ”€â”€ Code/
â”‚   â”œâ”€â”€ RQ1/                        # Domain Composition
â”‚   â”‚   â”œâ”€â”€ EVOL/
â”‚   â”‚   â”œâ”€â”€ OSS/
â”‚   â”‚   â””â”€â”€ domain_composition.py
â”‚   â”œâ”€â”€ RQ2/                        # Semantic Diversity
â”‚   â”‚   â”œâ”€â”€ EVOL/
â”‚   â”‚   â”œâ”€â”€ OSS/
â”‚   â”‚   â”œâ”€â”€ evol_embeddings.pickle
â”‚   â”‚   â”œâ”€â”€ oss_embeddings.pickle
â”‚   â”‚   â””â”€â”€ semantic_diversity.py
â”‚   â”œâ”€â”€ RQ3/                        # Problem Complexity
â”‚   â”‚   â”œâ”€â”€ EVOL/
â”‚   â”‚   â”œâ”€â”€ OSS/
â”‚   â”‚   â””â”€â”€ problem_complexity.py
â”‚   â”œâ”€â”€ RQ4/                        # Task-type Coverage
â”‚   â”‚   â”œâ”€â”€ EVOL/
â”‚   â”‚   â”œâ”€â”€ OSS/
â”‚   â”‚   â”œâ”€â”€ evol_instruct_with_category.jsonl
â”‚   â”‚   â”œâ”€â”€ oss_instruct_with_category.jsonl
â”‚   â”‚   â”œâ”€â”€ task_category.py
â”‚   â”‚   â””â”€â”€ type_coverage.py
â”œâ”€â”€ Datas/                          # Raw and processed instruction datasets


### Model Training and Inference

We use the [LLaMA-Factory library](https://github.com/hiyouga/LLaMA-Factory) for the training and inference process. 


### Evaluation

We evaluate the fine-tuned models using the [BigCode Evaluation Harness](https://github.com/bigcode-project/bigcode-evaluation-harness), a standard toolkit for benchmarking code LLMs across multiple programming languages and tasks.

